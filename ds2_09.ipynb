{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ds2_09.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## テキストの前処理\n",
        "[janome](https://mocobeta.github.io/janome/)を利用する  \n",
        "*   インストール \n",
        "*   形態素解析\n",
        "*   品詞によるフィルタリング \n",
        "*   [ストップワード](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)の除去\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3vn16LNsjCOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E13Ue7TIi_dI"
      },
      "outputs": [],
      "source": [
        "!pip install janome  #インストール"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#形態素解析\n",
        "from janome.tokenizer import Tokenizer\n",
        "text = 'なんて美しい風景でしょう！感動した！！'\n",
        "\n",
        "t = Tokenizer() #何も指定しないと，すべての情報を出力\n",
        "for token in t.tokenize(text):\n",
        "  print(token)\n",
        "\n",
        "print('------------------------')\n",
        "\n",
        "t = Tokenizer(wakati = True) #wakati = Trueを指定すると分かち書きを行う\n",
        "result = [t for t in t.tokenize(text)]  \n",
        "print( result )"
      ],
      "metadata": {
        "id": "mOFBgVnHlbgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#品詞によるフィルタリング\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.tokenfilter import POSKeepFilter, POSStopFilter #POS = PartOfSpeech = 品詞\n",
        "\n",
        "text = 'なんて美しい風景でしょう！感動した！！'\n",
        "\n",
        "#フィルタの定義\n",
        "filters = [] #フィルタなし --> 全部を取り出す\n",
        "#filters = [ POSKeepFilter( ['名詞', '動詞'] )] #PosKeepFilter：キープする（残す）品詞を指定する\n",
        "#filters = [ POSStopFilter( '記号' )] #StopFilter：止める（捨てる）品詞を指定する\n",
        "\n",
        "analyzer = Analyzer(token_filters = filters)\n",
        "for token in analyzer.analyze(text):\n",
        "  print(token)  "
      ],
      "metadata": {
        "id": "07wLhMTMl-O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stop word辞書の獲得\n",
        "!wget http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt ."
      ],
      "metadata": {
        "id": "mk9VMsZwnn_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ストップワードの除去\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.tokenfilter import POSKeepFilter, POSStopFilter #POS = PartOfSpeech\n",
        "\n",
        "# stop word辞書の読み込み（作業ディレクトリにJapanese.txtがあることを前提としている）\n",
        "def load_stopword_dictionary() : \n",
        "  with open('Japanese.txt', 'r', encoding='utf-8') as f :\n",
        "    stopwords = [w.strip() for w in f ]\n",
        "    stopwords = set(stopwords)\n",
        "  return stopwords\n",
        "\n",
        "# 除去関数の定義\n",
        "def remove_stopwords( words, stopwords ):\n",
        "  remained_words = [ w for w in words if w not in stopwords]\n",
        "  return remained_words\n",
        "\n",
        "stopwords = load_stopword_dictionary()\n",
        "text = 'なんて美しい風景でしょう！感動した！！'\n",
        "filters = [ POSKeepFilter( ['名詞'] )] #名詞だけを取り出す\n",
        "#filters = [ ] \n",
        "\n",
        "analyzer = Analyzer(token_filters = filters)\n",
        "words = [w.surface for w in analyzer.analyze(text)]\n",
        "\n",
        "words = remove_stopwords(words, stopwords)\n",
        "print( words )\n"
      ],
      "metadata": {
        "id": "9MKYqgNrmIes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## カウントベースによる単語ベクトル化\n",
        "*   準備：関数の定義\n",
        "*   データ（[羅生門](https://www.aozora.gr.jp/cards/000879/card127.html)）の読み込みと共起行列・PPMI行列の作成\n",
        "\n",
        "*   類似単語の検索・表示\n"
      ],
      "metadata": {
        "id": "plk0l2mloKcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#準備：関数の定義\n",
        "\n",
        "import numpy as np\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.tokenfilter import POSKeepFilter, POSStopFilter #POS = PartOfSpeech = 品詞\n",
        "\n",
        "# load_stopword_dictionary, remove_stopwordsは，先ほどと同じもの\n",
        "def load_stopword_dictionary() : \n",
        "  with open('Japanese.txt', 'r', encoding='utf-8') as f :\n",
        "    stopwords = [w.strip() for w in f ]\n",
        "    stopwords = set(stopwords)\n",
        "  return stopwords\n",
        "\n",
        "# 除去関数の定義\n",
        "def remove_stopwords( words, stopwords ):\n",
        "  remained_words = [ w for w in words if w not in stopwords]\n",
        "  return remained_words\n",
        "\n",
        "# 前処理用関数\n",
        "def preprocess(text, stopwords):\n",
        "  filters = [ POSKeepFilter( ['名詞', '形容詞', '動詞'] )] #ここは適当に合わせる\n",
        "  analyzer = Analyzer(token_filters = filters)\n",
        "  words = [w.surface for w in analyzer.analyze(text)]\n",
        "  words = remove_stopwords(words, stopwords) #stopword除去\n",
        "\n",
        "  word_to_id = {}\n",
        "  id_to_word = {}\n",
        "  for word in words:\n",
        "    if word not in word_to_id:\n",
        "      new_id = len(word_to_id)\n",
        "      word_to_id[word] = new_id\n",
        "      id_to_word[new_id] = word\n",
        "\n",
        "  corpus = np.array( [word_to_id[w] for w in words] )\n",
        "  return corpus, word_to_id, id_to_word\n",
        "\n",
        "#共起行列の構築\n",
        "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
        "  corpus_size = len(corpus)\n",
        "  co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "  for idx, word_id in enumerate(corpus):\n",
        "    for i in range(1, window_size + 1):\n",
        "      left_idx = idx - i\n",
        "      if left_idx >= 0:\n",
        "        left_word_id = corpus[left_idx]\n",
        "        co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "      right_idx = idx + i\n",
        "      if right_idx < corpus_size:\n",
        "        right_word_id = corpus[right_idx]\n",
        "        co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "  return co_matrix\n",
        "\n",
        "#共起行列CをPPMI行列Mへ変換\n",
        "def ppmi(C, eps = 1e-8):\n",
        "  M = np.zeros_like(C, dtype=np.float32)\n",
        "  N = np.sum(C)\n",
        "  S = np.sum(C, axis=0)\n",
        "  total = C.shape[0] * C.shape[1]\n",
        "  cnt = 0\n",
        "\n",
        "  for i in range(C.shape[0]):\n",
        "    for j in range(C.shape[1]):\n",
        "      pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
        "      M[i, j] = max(0, pmi)\n",
        "\n",
        "  return M\n",
        "\n",
        "#ベクトルx,y間のコサイン類似度\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "  nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
        "  ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
        "  return np.dot(nx, ny)\n",
        "\n",
        "#コサイン類似度上位top件の表示\n",
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "  if query not in word_to_id:\n",
        "    print('%s is not found' % query)\n",
        "    return\n",
        "\n",
        "  print('\\n[query] ' + query)\n",
        "  query_id = word_to_id[query]\n",
        "  query_vec = word_matrix[query_id]\n",
        "\n",
        "  vocab_size = len(id_to_word)\n",
        "\n",
        "  similarity = np.zeros(vocab_size)\n",
        "  for i in range(vocab_size):\n",
        "    similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "  count = 0\n",
        "  for i in (-1 * similarity).argsort():\n",
        "    if id_to_word[i] == query:\n",
        "      continue\n",
        "    print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "    count += 1\n",
        "    if count >= top:\n",
        "      return\n"
      ],
      "metadata": {
        "id": "nwbjDTjgnqz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ファイルの読み込みと共起行列・PPMI行列の作成\n",
        "\n",
        "#対象データの読み込み\n",
        "#作業ディレクトリにデータ（rashomon.txt）が配置されていることを前提\n",
        "with open('rashomon.txt', 'r', encoding='utf-8') as f :\n",
        "  text = [w.strip() for w in f]\n",
        "document = ' '.join(text) #対象テキスト\n",
        "\n",
        "stopwords = load_stopword_dictionary() #stopword辞書の準備\n",
        "\n",
        "#コーパスの構築\n",
        "#curpus : documentを単語idに変換したもの\n",
        "#word_to_id：単語 --> ID 辞書\n",
        "#id_to_word : ID --> 単語 辞書\n",
        "(corpus, word_to_id, id_to_word) = preprocess(document, stopwords) \n",
        "\n",
        "#共起行列の構築，window_sizeで「文脈」の範囲を指定\n",
        "co_matrix = create_co_matrix(corpus, len(word_to_id), window_size = 4)\n",
        "ppmi_matrix = ppmi( co_matrix ) #PPMI行列の構築\n",
        "\n",
        "print( np.shape( ppmi_matrix ))  #単語数の確認\n",
        "print( word_to_id ) #単語の確認\n",
        "\n"
      ],
      "metadata": {
        "id": "Mxa91qbUntL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#類似単語を検索してみよう！　結果は微妙？．．\n",
        "most_similar('老婆', word_to_id, id_to_word, co_matrix)\n",
        "most_similar('風', word_to_id, id_to_word, co_matrix)\n",
        "most_similar('胡麻', word_to_id, id_to_word, co_matrix)\n",
        "most_similar('今日', word_to_id, id_to_word, co_matrix)\n",
        "\n"
      ],
      "metadata": {
        "id": "1VlYlTIwrv_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 推論ベースによる単語ベクトル化\n",
        "*   学習済みモデル（[Facebook公開のfastText](https://fasttext.cc/)）のダウンロード\n",
        "*   学習済みモデル（[Wikipedia Entity Vectors（w2vモデル）](https://github.com/singletongue/WikiEntVec/)）のダウンロード\n",
        "*  モデルの読み込み\n",
        "*  モデルの利用  \n",
        "\n"
      ],
      "metadata": {
        "id": "Kh7FWYJgsv36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.25GBくらいあります（時間のある時に実行しましょう）\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz ."
      ],
      "metadata": {
        "id": "L4L0BTNOr7zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/singletongue/WikiEntVec/releases/download/20190520/jawiki.entity_vectors.100d.txt.bz2 ."
      ],
      "metadata": {
        "id": "b13muBLIq2TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#モデルの読み込み：5-10分程度かかります（時間のある時に試しましょう） facebook/wikipedia 任意で選択してください\n",
        "import gensim\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('cc.ja.300.vec.gz',binary=False) \n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('jawiki.entity_vectors.100d.txt.bz2',binary=False) \n"
      ],
      "metadata": {
        "id": "SSaPZmdgt3Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#モデルの利用1 単語間類似度の計算\n",
        "model.similarity('犬','馬') "
      ],
      "metadata": {
        "id": "5XgyJ1JHuHrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#モデルの利用2 類似語の検索\n",
        "model.most_similar('科学')  "
      ],
      "metadata": {
        "id": "s7GgdJnCvyQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#モデルの利用3 類推（ベクトル計算）\n",
        "# positiveに含まれる単語ベクトルの和 - negativeに含まれる単語ベクトルの和\n",
        "model.most_similar(positive=['王様','女性'],negative=['男性'], topn=5) #（王様 + 女性 - 男性=??）"
      ],
      "metadata": {
        "id": "LkyFVJJfwI7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2qnyHtp9wMWo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}