{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ7qlP_zlqUV"
      },
      "source": [
        "## カウントベースによる手法：TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lgclQJbUnEdF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: janome in /Users/Buntin-LArchel/opt/anaconda3/lib/python3.9/site-packages (0.4.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install janome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QRDKzCwhlpj9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('あと', 0), ('みんな', 1), ('イタチ', 2), ('キツネ', 3), ('ネズミ', 4), ('一', 5), ('他', 6), ('兎', 7), ('匹', 8), ('名前', 9), ('吾輩', 10), ('犬', 11), ('猫', 12), ('猿', 13), ('立派', 14), ('足', 15)]\n",
            "[[0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0]\n",
            " [0 0 0 0 0 3 0 1 3 0 0 1 1 1 1 0]\n",
            " [3 1 1 1 1 0 1 1 0 0 0 0 1 0 0 3]]\n",
            " ---- \n",
            "[('あと', 0), ('みんな', 1), ('イタチ', 2), ('キツネ', 3), ('ネズミ', 4), ('一', 5), ('他', 6), ('兎', 7), ('匹', 8), ('名前', 9), ('吾輩', 10), ('犬', 11), ('猫', 12), ('猿', 13), ('立派', 14), ('足', 15)]\n",
            "[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.67 0.67 0.   0.32 0.\n",
            "  0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.64 0.   0.14 0.64 0.   0.   0.21 0.1  0.21\n",
            "  0.21 0.  ]\n",
            " [0.62 0.21 0.21 0.21 0.21 0.   0.21 0.14 0.   0.   0.   0.   0.1  0.\n",
            "  0.   0.62]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.tokenfilter import POSKeepFilter\n",
        "\n",
        "#日本語テキストを分かち書きするクラス\n",
        "class WordDividor:\n",
        "  def extract_words(self, text):\n",
        "    filters = [ POSKeepFilter( ['名詞'] )]\n",
        "    analyzer = Analyzer(token_filters = filters)\n",
        "    return [token.surface for token in analyzer.analyze(text)]\n",
        "\n",
        "#対象文書\n",
        "docs = [\n",
        "  '吾輩は猫である．名前はまだない',\n",
        "  '一匹の立派な犬や兎や一匹の子猿や一匹の猫などを飼った',\n",
        "  '兎やキツネの他に，イタチの足あと，ネズミの足あと，猫の足あと，みんなちがう'  \n",
        "]\n",
        "\n",
        "tokenizer = WordDividor() #tokenizerの生成\n",
        "\n",
        "#countエンコーディング (binary=Trueの場合はOne-Hotエンコーディング)\n",
        "vectorizer = CountVectorizer( binary=False, tokenizer=tokenizer.extract_words )\n",
        "bow = vectorizer.fit_transform( docs ) \n",
        "#print(vectorizer.vocabulary_)\n",
        "print( sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1]) )\n",
        "print( bow.toarray() )\n",
        "\n",
        "print( ' ---- ')\n",
        "\n",
        "#tfidf\n",
        "tfidf = TfidfVectorizer(tokenizer=tokenizer.extract_words, smooth_idf=False)\n",
        "tfidf_vector = tfidf.fit_transform(docs)\n",
        "#print(tfidf.vocabulary_)\n",
        "print( sorted(tfidf.vocabulary_.items(), key=lambda x:x[1]) )\n",
        "print( tfidf_vector.toarray().round(2) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mcIYfLHIbcO"
      },
      "source": [
        "## LDA（参考）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mWf3ce8BIz1y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: janome in /Users/Buntin-LArchel/opt/anaconda3/lib/python3.9/site-packages (0.4.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/Buntin-LArchel/opt/anaconda3/lib/python3.9/site-packages (4.11.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/Buntin-LArchel/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\n"
          ]
        }
      ],
      "source": [
        "#必要なライブラリのインストール\n",
        "!pip install janome beautifulsoup4  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5sN28nmUThFz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-03 23:45:14--  https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\n",
            "s3.amazonaws.com (s3.amazonaws.com) をDNSに問いあわせています... 52.216.114.77\n",
            "s3.amazonaws.com (s3.amazonaws.com)|52.216.114.77|:443 に接続しています... 接続しました。\n",
            "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
            "長さ: 94688992 (90M) [application/x-gzip]\n",
            "`amazon_reviews_multilingual_JP_v1_00.tsv.gz.1' に保存中\n",
            "\n",
            "amazon_reviews_mult 100%[===================>]  90.30M  7.31MB/s 時間 12s        \n",
            "\n",
            "2022-07-03 23:45:27 (7.56 MB/s) - `amazon_reviews_multilingual_JP_v1_00.tsv.gz.1' へ保存完了 [94688992/94688992]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#データのダウンロード\n",
        "!wget https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V9HAc05vJBHv"
      },
      "outputs": [
        {
          "ename": "EOFError",
          "evalue": "Compressed file ended before the end-of-stream marker was reached",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-日本大学/NihonUniv/Jupyter/ds2_10.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-%E6%97%A5%E6%9C%AC%E5%A4%A7%E5%AD%A6/NihonUniv/Jupyter/ds2_10.ipynb#ch0000006?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [token\u001b[39m.\u001b[39msurface \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manalyzer\u001b[39m.\u001b[39manalyze(text)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-%E6%97%A5%E6%9C%AC%E5%A4%A7%E5%AD%A6/NihonUniv/Jupyter/ds2_10.ipynb#ch0000006?line=26'>27</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mamazon_reviews_multilingual_JP_v1_00.tsv.gz\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-%E6%97%A5%E6%9C%AC%E5%A4%A7%E5%AD%A6/NihonUniv/Jupyter/ds2_10.ipynb#ch0000006?line=27'>28</a>\u001b[0m reviews, ratings \u001b[39m=\u001b[39m load_dataset(url, n\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m) \u001b[39m#各クラスごとにn=1000件を選択\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-%E6%97%A5%E6%9C%AC%E5%A4%A7%E5%AD%A6/NihonUniv/Jupyter/ds2_10.ipynb#ch0000006?line=29'>30</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m Preprocessor() \u001b[39m#tokenizerの生成\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-%E6%97%A5%E6%9C%AC%E5%A4%A7%E5%AD%A6/NihonUniv/Jupyter/ds2_10.ipynb#ch0000006?line=30'>31</a>\u001b[0m \u001b[39m#countエンコーディング (df >= 0.3なる単語は排除，頻度上位100件)\u001b[39;00m\n",
            "\u001b[1;32m/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-日本大学/NihonUniv/Jupyter/ds2_10.ipynb Cell 7'\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(filename, n, random_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-%E6%97%A5%E6%9C%AC%E5%A4%A7%E5%AD%A6/NihonUniv/Jupyter/ds2_10.ipynb#ch0000006?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_dataset\u001b[39m(filename, n\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-%E6%97%A5%E6%9C%AC%E5%A4%A7%E5%AD%A6/NihonUniv/Jupyter/ds2_10.ipynb#ch0000006?line=10'>11</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(filename, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m) \u001b[39m#データ読み込み\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-%E6%97%A5%E6%9C%AC%E5%A4%A7%E5%AD%A6/NihonUniv/Jupyter/ds2_10.ipynb#ch0000006?line=11'>12</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39mrandom_state)  \u001b[39m# shuffle\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Buntin-LArchel/Library/CloudStorage/OneDrive-%E6%97%A5%E6%9C%AC%E5%A4%A7%E5%AD%A6/NihonUniv/Jupyter/ds2_10.ipynb#ch0000006?line=12'>13</a>\u001b[0m     grouped \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mstar_rating\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m#各ratingでn件のレビューを得る\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1252\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1253\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[1;32m   1255\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    226\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1952\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/gzip.py:506\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     \u001b[39mif\u001b[39;00m buf \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 506\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCompressed file ended before the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    507\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mend-of-stream marker was reached\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_read_data( uncompress )\n\u001b[1;32m    510\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(uncompress)\n",
            "\u001b[0;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"
          ]
        }
      ],
      "source": [
        "#データの読み込みと前処理\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from janome.tokenizer import Tokenizer\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.tokenfilter import POSKeepFilter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#データの読み込みとフィルタリング\n",
        "def load_dataset(filename, n=1000, random_state = 42):\n",
        "    df = pd.read_csv(filename, sep='\\t') #データ読み込み\n",
        "    df = df.sample(frac=1, random_state=random_state)  # shuffle\n",
        "    grouped = df.groupby('star_rating') #各ratingでn件のレビューを得る\n",
        "    df = grouped.head(n=n)\n",
        "    return df.review_body.values, df.star_rating.values\n",
        "\n",
        "class Preprocessor:\n",
        "  def __init__(self):\n",
        "    self.filters = [ POSKeepFilter( ['名詞','形容詞','副詞'] )]\n",
        "    self.analyzer = Analyzer(token_filters = self.filters)\n",
        "\n",
        "  def extract_words(self, text):\n",
        "    soup = BeautifulSoup(text, 'html.parser') #htmlの除去\n",
        "    text = soup.get_text(strip=True)\n",
        "    return [token.surface for token in self.analyzer.analyze(text)]\n",
        "\n",
        "url = 'amazon_reviews_multilingual_JP_v1_00.tsv.gz'\n",
        "reviews, ratings = load_dataset(url, n=1000) #各クラスごとにn=1000件を選択\n",
        "\n",
        "tokenizer = Preprocessor() #tokenizerの生成\n",
        "#countエンコーディング (df >= 0.3なる単語は排除，頻度上位100件)\n",
        "counter = CountVectorizer( binary=False, tokenizer=tokenizer.extract_words, max_features=100, max_df=0.3 )\n",
        "review_bow = counter.fit_transform( reviews ) #bow表現へ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0OQ2yniNsNA"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "n_topics = 10  #tトピック数の設定\n",
        "lda_model = LatentDirichletAllocation( n_components = n_topics)\n",
        "lda_model.fit( review_bow ) #LDAの実行\n",
        "review_vec = lda_model.transform(review_bow) #各文書に対するベクトル\n",
        "#print( review_vec.shape )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6ZTNDieIgGE"
      },
      "outputs": [],
      "source": [
        "n_of_words = 10\n",
        "feature_names = counter.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda_model.components_) :\n",
        "  print(\"Topic %d:\" %(topic_idx+1) ,end=\" \")\n",
        "  ws = [ feature_names[i] for i in topic.argsort()[:-n_of_words-1 : -1]]\n",
        "  print(\" \".join(ws))\n",
        "\n",
        "print()\n",
        "target_topic = 3\n",
        "for movie_idx in (review_vec[:,target_topic].argsort()[::-1])[:3] :\n",
        "  print(reviews[movie_idx])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSSctFTqodej"
      },
      "source": [
        "## word2vecベクトルの利用：swem\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeKqQoGBnXy_"
      },
      "outputs": [],
      "source": [
        "# 前回ダウンロードしています．残っていれば，改めてDLする必要はありません\n",
        "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz .\n",
        "# !wget https://github.com/singletongue/WikiEntVec/releases/download/20190520/jawiki.entity_vectors.100d.txt.bz2 .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQKQL_6xo-Gd"
      },
      "outputs": [],
      "source": [
        "#必要なライブラリとswemのインストール\n",
        "!pip install mecab-python3==0.996.2 #形態素解析器 Mecabのインストール\n",
        "!git clone https://github.com/yagays/swem.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYFCnSc_p_CU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(\"swem\")\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from swem import MeCabTokenizer\n",
        "from swem import SWEM\n",
        "\n",
        "#w2v_path = \"/path/to/word_embedding.bin\" #word2vecのファイルを指定する\n",
        "w2v_path = \"jawiki.entity_vectors.100d.txt.bz2\" #自身の環境に合わせて，パスを変更\n",
        "\n",
        "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=False) #読み込みに時間がかかります\n",
        "tokenizer = MeCabTokenizer(\"-O wakati\")\n",
        "\n",
        "swem = SWEM(w2v, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxKehaJGrwpV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "docs = [\n",
        "  '吾輩は猫である．名前はまだない',\n",
        "  '一匹の立派な犬や兎や一匹の子猿や一匹の猫などを飼った',\n",
        "  '兎やキツネの他に，イタチの足あと，ネズミの足あと，猫の足あと，みんなちがう'  \n",
        "]\n",
        "vec_avgs = [ swem.average_pooling(d) for d in docs ]\n",
        "vec_maxs = [ swem.max_pooling(d) for d in docs ]\n",
        "vec_concats = [ swem.concat_average_max_pooling(d) for d in docs]\n",
        "vec_hiers = [ swem.hierarchical_pooling(d, n=3) for d in docs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR37ObLUxcHl"
      },
      "outputs": [],
      "source": [
        "def cos_similarity(x, y, eps=1e-8):\n",
        "  nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
        "  ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
        "  return np.dot(nx, ny)\n",
        "\n",
        "for i in range(len(docs)):\n",
        "  cos_avg = cos_similarity(vec_avgs[i], vec_avgs[(i+1)%len(docs)] )\n",
        "  cos_max = cos_similarity(vec_maxs[i], vec_maxs[(i+1)%len(docs)] )\n",
        "  cos_con = cos_similarity(vec_concats[i], vec_concats[(i+1)%len(docs)] )\n",
        "  cos_hie = cos_similarity(vec_hiers[i], vec_hiers[(i+1)%len(docs)] )\n",
        "  print(i, (i+1), cos_avg, cos_max, cos_con, cos_hie)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPihPAlx3I22"
      },
      "source": [
        "## 推論ベースの手法:doc2vec (参考)\n",
        "[日本語WIKIPEDIAで学習したDOC2VECモデル](https://yag-ays.github.io/project/pretrained_doc2vec_wikipedia/)より  \n",
        "モデルが大きいです．無理に動かす必要はありません\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdEQ5dg10Vtx"
      },
      "outputs": [],
      "source": [
        "#dbowモデル(5.48GB)を獲得\n",
        "!wget https://www.dropbox.com/s/j75s0eq4eeuyt5n/jawiki.doc2vec.dbow300d.tar.bz2 .  #dl\n",
        "!tar xfvj jawiki.doc2vec.dbow300d.tar.bz2 #解凍"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yXJb1YH65sH"
      },
      "outputs": [],
      "source": [
        "import MeCab\n",
        "def tokenize(text):\n",
        "    wakati = MeCab.Tagger(\"-O wakati\")\n",
        "    wakati.parse(\"\")\n",
        "    return wakati.parse(text).strip().split()\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "model_path = \"jawiki.doc2vec.dbow300d.model\" #自身の環境に合わせて，パスを変更\n",
        "model = Doc2Vec.load(model_path) #モデルの読み込み 時間がかかります"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_1zUTaQDqxj"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "吾輩は猫である．名前はまだない．\n",
        "\"\"\"\n",
        "vec = model.infer_vector(tokenize(text)) #文を分かち書きし，ベクトルを推定する（文書ベクトルの獲得）\n",
        "model.docvecs.most_similar([vec]) #得られたベクトルを使って類似センテンスを獲得"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JFvQ2c0NmU"
      },
      "source": [
        "## 推論ベースの手法：BERT\n",
        "モデルのロードと文書ベクトルの獲得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpZKWVmlE_ev"
      },
      "outputs": [],
      "source": [
        "!pip install transformers fugashi ipadic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhgiQ32ZEwQV"
      },
      "outputs": [],
      "source": [
        "from transformers import BertJapaneseTokenizer, BertModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#初回実行時にモデルをダウンロードします\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPkMtQkOSq1d"
      },
      "outputs": [],
      "source": [
        "# 文書ベクトル獲得メソッド\n",
        "def get_bert_vectors( model, sentences) :\n",
        "  encoded_data = tokenizer.batch_encode_plus(sentences, padding='longest') \n",
        "  input_ids = torch.tensor(encoded_data[\"input_ids\"])\n",
        "\n",
        "  with torch.no_grad(): # 勾配計算を行わない\n",
        "    outputs = model( input_ids )\n",
        "  sentence_vecs = outputs[0][:,0,:] #最終層の重みを獲得\n",
        "  sentence_vecs = sentence_vecs.to('cpu').detach().numpy().copy() #numpy配列に変換\n",
        "  return sentence_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpPeH8JVRXCR"
      },
      "outputs": [],
      "source": [
        "#ベクトルの獲得例\n",
        "sentences = [ \n",
        "  '吾輩は猫である．名前はまだない',\n",
        "  '一匹の立派な犬や兎や一匹の子猿や一匹の猫などを飼った',\n",
        "  '兎やキツネの他に，イタチの足あと，ネズミの足あと，猫の足あと，みんなちがう'\n",
        "]\n",
        "\n",
        "result = get_bert_vectors(model, sentences)\n",
        "\n",
        "print( np.shape( result ) ) #各文が768次元ベクトルに\n",
        "print( result )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ds2_10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "bb9919a6b28bd75a67944889238d44d1912a3caba5e3708e077a7c1b56416040"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
