{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ds2_ssl.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1u5FIsvi3c0qh9ym0UlUAnZBuuZVuvg8s","authorship_tag":"ABX9TyP197V5caEe4/kXXOR8a7xM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## データの準備\n","[Amazon Customer Reviews Dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)で提供されている日本語レビューを利用する"],"metadata":{"id":"PXKZ5FA3ooCE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BM6rjxjIobfD"},"outputs":[],"source":["#データのダウンロード：既にダウンロード済みの場合は、改めてダウンロードする必要はない\n","!wget https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz ."]},{"cell_type":"code","source":["!pip install beautifulsoup4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvStAtCSnPfl","executionInfo":{"status":"ok","timestamp":1655872793543,"user_tz":-540,"elapsed":5365,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}},"outputId":"404910cd-e1b6-42d6-f2b2-da7e26bf1b96"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from bs4 import BeautifulSoup\n","\n","from sklearn.model_selection import train_test_split\n","def load_dataset(filename, n = 1000, test_size = 0.2, random_state = 42) :\n","    df = pd.read_csv(filename, sep='\\t') #データ読み込み\n","    df = df.sample(frac=1, random_state=random_state)  # shuffle\n","    grouped = df.groupby('star_rating') #各ratingでn件のレビューを得る\n","    df = grouped.head(n=n)\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        df.review_body.values, df.star_rating.values, \n","        test_size=test_size, random_state=42)\n","    \n","    X_train = np.array( [BeautifulSoup(text, 'html.parser').get_text() for text in X_train ])\n","    X_test = np.array( [BeautifulSoup(text, 'html.parser').get_text() for text in X_test ])\n","    return X_train, X_test, y_train, y_test\n","\n","#以降のパラメタを適切に設定する\n","path_to_file = \"amazon_reviews_multilingual_JP_v1_00.tsv.gz\"\n","n = 1000           #クラスごとのデータ数\n","test_size = 0.2   #分割割合\n","random_state = 42 #乱数の種\n","\n","X_train, X_test, y_train, y_test = load_dataset(path_to_file, n, test_size, random_state)\n","    \n","print( \"サイズ：訓練例Xy = ({},{})，テスト例Xy = ({},{})\".format( len(X_train), len(y_train), len(X_test), len(y_test) ) )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBigZJKmpq0v","executionInfo":{"status":"ok","timestamp":1655877727731,"user_tz":-540,"elapsed":11106,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}},"outputId":"d42c0a3d-2f88-4ed4-e94b-98a0a72c4749"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["サイズ：訓練例Xy = (4000,4000)，テスト例Xy = (1000,1000)\n"]}]},{"cell_type":"markdown","source":["## TF-IDFによるベクトル化"],"metadata":{"id":"SONP4oNhr0Vg"}},{"cell_type":"code","source":["\n","!pip install mecab-python3==0.996.2 #形態素解析器 Mecabのインストール"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J7UMvJ2arimt","executionInfo":{"status":"ok","timestamp":1655873440321,"user_tz":-540,"elapsed":4678,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}},"outputId":"c325fc53-595d-47ea-b78e-b22b4d4953a7"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mecab-python3==0.996.2 in /usr/local/lib/python3.7/dist-packages (0.996.2)\n"]}]},{"cell_type":"code","source":["import MeCab\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","class Preprocessor:\n","  def __init__(self):\n","    self.pos_filter = [ '名詞', '動詞', '形容詞' ]\n","    self.tagger = MeCab.Tagger()\n","    self.tagger.parse(\"\")\n","\n","  def extract_words(self, text):\n","    node = self.tagger.parseToNode(text)\n","    terms = []\n","    while node:\n","        term = node.surface\n","        pos = node.feature.split(',')[0]\n","        if pos in self.pos_filter:\n","            terms.append(term)\n","        node = node.next\n","    text_result = ' '.join(terms)\n","    return text_result\n","\n","def get_tfidf_vectors(train, test, max_features=100, max_df = 0.3) :\n","  tfidf = TfidfVectorizer(tokenizer=Preprocessor().extract_words, smooth_idf=False, \n","                          max_features = max_features, max_df = max_df)\n","  train_tfidf =  tfidf.fit_transform(train).toarray()\n","  test_tfidf = tfidf.transform(test).toarray()\n","  return train_tfidf, test_tfidf\n","\n","#TFIDFベクトルの獲得\n","max_features = 100\n","max_df = 0.3\n","train_tfidf, test_tfidf = get_tfidf_vectors(X_train, X_test, max_features, max_df)\n","\n","print( \"サイズ：訓練例X = {}，テスト例X = {}\".format( train_tfidf.shape, test_tfidf.shape) )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u89HTQ8btLEd","executionInfo":{"status":"ok","timestamp":1655877735313,"user_tz":-540,"elapsed":3491,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}},"outputId":"dda24ed1-317e-4407-deda-df48bd24cbb4"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["サイズ：訓練例X = (4000, 100)，テスト例X = (1000, 100)\n"]}]},{"cell_type":"markdown","source":["##LDAによるベクトル化"],"metadata":{"id":"oHbmpPyUw7ng"}},{"cell_type":"code","source":["!pip install mecab-python3==0.996.2 "],"metadata":{"id":"sKc-y5HrwdaL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import MeCab\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","# TfIdfにおける前処理器と同一\n","class Preprocessor:\n","  def __init__(self):\n","    self.pos_filter = [ '名詞', '動詞', '形容詞' ]\n","    self.tagger = MeCab.Tagger()\n","    self.tagger.parse(\"\")\n","\n","  def extract_words(self, text):\n","    node = self.tagger.parseToNode(text)\n","    terms = []\n","    while node:\n","        term = node.surface\n","        pos = node.feature.split(',')[0]\n","        if pos in self.pos_filter:\n","            terms.append(term)\n","        node = node.next\n","    text_result = ' '.join(terms)\n","    return text_result\n","\n","def get_lda_vectors(train, test, n_topics = 10, max_features=100, max_df = 0.3) :\n","  counter = CountVectorizer(tokenizer=Preprocessor().extract_words,\n","                          max_features = max_features, max_df = max_df)\n","  train_bow =  counter.fit_transform(train)\n","  test_bow = counter.transform(test)\n","  lda_model = LatentDirichletAllocation( n_components = n_topics)\n","  train_lda = lda_model.fit_transform( train_bow ) #LDAの実行\n","  test_lda = lda_model.transform(test_bow)\n","\n","  return train_lda, test_lda\n","\n","#LDAベクトルの獲得\n","n_topics = 10\n","max_features = 100\n","max_df = 0.3\n","\n","train_lda, test_lda = get_lda_vectors(X_train, X_test, n_topics, max_features, max_df)\n","\n","print( \"サイズ：訓練例X = {}，テスト例X = {}\".format( train_lda.shape, test_lda.shape) )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"euaVIxmcxN4m","executionInfo":{"status":"ok","timestamp":1655877766074,"user_tz":-540,"elapsed":24393,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}},"outputId":"fb210f1a-9507-42c4-ee61-a784fac64d12"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["サイズ：訓練例X = (4000, 10)，テスト例X = (1000, 10)\n"]}]},{"cell_type":"markdown","source":["## word2vecを利用したベクトル化(SWEM)"],"metadata":{"id":"0fhhept9y4yX"}},{"cell_type":"code","source":["#データのダウンロード：既にダウンロード済みの場合は、改めてダウンロードする必要はない\n","!wget https://github.com/singletongue/WikiEntVec/releases/download/20190520/jawiki.entity_vectors.100d.txt.bz2 ."],"metadata":{"id":"puS-SNUCyVIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#必要なライブラリとswemのインストール：インストール済みであれば実行する必要はない\n","!pip install mecab-python3==0.996.2 #形態素解析器 Mecabのインストール\n","!git clone https://github.com/yagays/swem.git"],"metadata":{"id":"8wqmOmoqzO8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import os\n","import sys\n","sys.path.append(\"swem\")\n","\n","from gensim.models import KeyedVectors\n","from swem import MeCabTokenizer\n","from swem import SWEM\n","\n","#w2v_path = \"/path/to/word_embedding.bin\" #word2vecのファイルを指定する\n","w2v_path = \"jawiki.entity_vectors.100d.txt.bz2\" #自身の環境に合わせて，パスを変更\n","\n","w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=False) #読み込みに時間がかかります\n","tokenizer = MeCabTokenizer(\"-O wakati\")\n","\n","swem = SWEM(w2v, tokenizer)\n","\n","def get_swem_avg_vectors(train, test):\n","  train_swemAvg = np.array( [swem.average_pooling(text) for text in train] )\n","  test_swemAvg = np.array( [swem.average_pooling(text) for text in test] )\n","  return train_swemAvg, test_swemAvg\n","\n","def get_swem_max_vectors(train, test):\n","  train_swemMax = np.array( [swem.max_pooling(text) for text in train] )\n","  test_swemMax = np.array( [swem.max_pooling(text) for text in test] )\n","  return train_swemMax, test_swemMax\n","\n","def get_swem_concat_vectors(train, test):\n","  train_swemConcat = np.array( [swem.concat_average_max_pooling(text) for text in train] )\n","  test_swemConcat = np.array( [swem.concat_average_max_pooling(text) for text in test] )\n","  return train_swemConcat, test_swemConcat\n","\n","def get_wwem_hier_vectors(train, test, n = 3):\n","  train_swemHier = np.array( [swem.hierarchical_pooling(text, n=3) for text in train] )\n","  test_swemHier = np.array( [swem.hierarchical_pooling(text, n=3) for text in test] )\n","  return train_swemHier, test_swemHier\n","\n"],"metadata":{"id":"_dCT7RRozVV5","executionInfo":{"status":"ok","timestamp":1655875677541,"user_tz":-540,"elapsed":193739,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["#swemベクトルの獲得\n","train_swemAvg, test_swemAvg = get_swem_avg_vectors(X_train, X_test)\n","print( \"サイズ：訓練例X = {}，テスト例X = {}\".format( train_swemAvg.shape, test_swemAvg.shape) )\n","\n","train_swemMax, test_swemMax = get_swem_max_vectors(X_train, X_test)\n","print( \"サイズ：訓練例X = {}，テスト例X = {}\".format( train_swemMax.shape, test_swemMax.shape) )\n","\n","train_swemConcat, test_swemConcat = get_swem_concat_vectors(X_train, X_test)\n","print( \"サイズ：訓練例X = {}，テスト例X = {}\".format( train_swemConcat.shape, test_swemConcat.shape) )\n","\n","n = 3\n","train_swemHier, test_swemHier = get_wwem_hier_vectors(X_train, X_test, n )\n","print( \"サイズ：訓練例X = {}，テスト例X = {}\".format( train_swemHier.shape, test_swemHier.shape) )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eYJmu3BS0oZQ","executionInfo":{"status":"ok","timestamp":1655877806925,"user_tz":-540,"elapsed":35771,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}},"outputId":"2b754b08-4081-48c4-edd9-f1825536e06f"},"execution_count":103,"outputs":[{"output_type":"stream","name":"stdout","text":["サイズ：訓練例X = (4000, 100)，テスト例X = (1000, 100)\n","サイズ：訓練例X = (4000, 100)，テスト例X = (1000, 100)\n","サイズ：訓練例X = (4000, 200)，テスト例X = (1000, 200)\n","サイズ：訓練例X = (4000, 100)，テスト例X = (1000, 100)\n"]}]},{"cell_type":"markdown","source":["## BERTによるベクトル化"],"metadata":{"id":"82dFtKuT5e9M"}},{"cell_type":"code","source":["!pip install transformers fugashi ipadic"],"metadata":{"id":"AmLnv-DA4Vw-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655872920316,"user_tz":-540,"elapsed":4630,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}},"outputId":"fc23fecd-cb3a-4e37-cbcb-a549aa63531b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n","Requirement already satisfied: fugashi in /usr/local/lib/python3.7/dist-packages (1.1.2)\n","Requirement already satisfied: ipadic in /usr/local/lib/python3.7/dist-packages (1.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"]}]},{"cell_type":"code","source":["from transformers import BertJapaneseTokenizer, BertModel\n","import pandas as pd\n","import numpy as np\n","import torch\n","\n","#初回実行時にモデルをダウンロードします\n","tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2GWWrh_46HG","executionInfo":{"status":"ok","timestamp":1655875717023,"user_tz":-540,"elapsed":6788,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}},"outputId":"896a0a8d-efe3-43f3-9adb-c49a8fd9dc7d"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["# 文書ベクトル獲得メソッド\n","def get_bert_vectors_batch( model, sentences, max_length = 300) :\n","  encoded_data = tokenizer.batch_encode_plus(sentences, max_length=max_length, padding=True, return_tensors=\"pt\", truncation=True) \n","  input_ids = torch.tensor(encoded_data[\"input_ids\"])\n","\n","  with torch.no_grad(): # 勾配計算を行わない\n","    outputs = model( input_ids )\n","  sentence_vecs = outputs[0][:,0,:] #最終層の重みを獲得\n","  sentence_vecs = sentence_vecs.to('cpu').detach().numpy().copy() #numpy配列に変換\n","  return sentence_vecs\n","\n","def get_sentence_vector( model, text, max_length = 300) :\n","  #print(text)\n","  input_ids = tokenizer.encode(text, max_length=max_length, padding=True, return_tensors=\"pt\", truncation=True) \n","  with torch.no_grad():\n","    outputs = model( input_ids )\n","  text_vec = outputs[0][:,0,:] [0]\n","  text_vec = text_vec.to('cpu').detach().numpy().copy() \n","  return text_vec\n","\n","def get_bert_vectors(model, train, test, max_length = 300):\n","  #train_bert = get_bert_vectors_batch(model, train, max_length)\n","  #test_bert = get_bert_vectors_batch(model, test, max_length)\n","\n","  train_bert = np.array([get_sentence_vector(model, text, max_length) for text in train])\n","  test_bert = np.array([get_sentence_vector(model, text, max_length) for text in test])\n","  return train_bert, test_bert\n","\n","#Bertベクトルの獲得(かなり時間がかかります。n=1000->全データ数5000で50分間)\n","train_bert, test_bert = get_bert_vectors(model, X_train, X_test)\n","\n","print( \"サイズ：訓練例X = {}，テスト例X = {}\".format( train_bert.shape, test_bert.shape) )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpnXdBpQiFdF","executionInfo":{"status":"ok","timestamp":1655880788764,"user_tz":-540,"elapsed":2961038,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}},"outputId":"3239c932-c26e-4ba5-a7fc-da7761e194e9"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["サイズ：訓練例X = (4000, 768)，テスト例X = (1000, 768)\n"]}]},{"cell_type":"markdown","source":["##分類による評価\n","例としてknn（K-近傍法）を用いて評価を行う  \n","k：近傍数\n","explained_variance_ratio：PCAを適用する際の累積寄与率"],"metadata":{"id":"VaIvqXD1x6o9"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"MrVFLWWTcS8b"}},{"cell_type":"code","source":["#KNN実行 & 評価の関数\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n","\n","def run_knn(X_train, X_test, y_train, y_test, k = 5, explained_variance_ratio=1) :\n","  if(explained_variance_ratio < 1) :\n","    pca = PCA(n_components = explained_variance_ratio)\n","    X_train = pca.fit_transform(X_train)\n","    X_test = pca.transform(X_test)\n","\n","  knn_clf = KNeighborsClassifier(n_neighbors = k)\n","  knn_clf.fit(X_train, y_train)\n","  y_pred = knn_clf.predict(X_test)\n","\n","  print(classification_report(y_test, y_pred))\n","  print('Precision:', precision_score(y_test, y_pred, average=None) )\n","  print('Recall:', recall_score(y_test, y_pred, average=None) )\n","  print('F1_score:', f1_score(y_test, y_pred, average=None) )\n","  print('Accuracy:', accuracy_score(y_test, y_pred) )\n"],"metadata":{"id":"hIzxrXu1Ay0b","executionInfo":{"status":"ok","timestamp":1655877698503,"user_tz":-540,"elapsed":356,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["train = train_bert #\n","test = test_bert   #\n","k = 20\n","explained_variance_ratio = 0.8 #1にセットするとPCAを適用しない\n","\n","run_knn(train, test, y_train, y_test, k, explained_variance_ratio) #"],"metadata":{"id":"nE0droO7By0C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655880886633,"user_tz":-540,"elapsed":1483,"user":{"displayName":"オザキトモノブ","userId":"09572397228778494355"}},"outputId":"c1d3f03c-c025-48f0-c4ed-af8798dd5b6f"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           1       0.43      0.57      0.49       203\n","           2       0.28      0.36      0.31       186\n","           3       0.26      0.21      0.23       204\n","           4       0.34      0.31      0.32       199\n","           5       0.52      0.35      0.42       208\n","\n","    accuracy                           0.36      1000\n","   macro avg       0.36      0.36      0.36      1000\n","weighted avg       0.37      0.36      0.36      1000\n","\n","Precision: [0.42592593 0.2780083  0.26060606 0.33695652 0.52142857]\n","Recall: [0.56650246 0.36021505 0.21078431 0.31155779 0.35096154]\n","F1_score: [0.48625793 0.31381733 0.23306233 0.32375979 0.41954023]\n","Accuracy: 0.36\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"qbLXSnEaAIpl"},"execution_count":null,"outputs":[]}]}